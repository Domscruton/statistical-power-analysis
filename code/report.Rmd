---
title: "Statistical Power Analysis"
author: "Dominic Scruton"
date: "December 2021"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r 1-initialize-session, echo = FALSE, message = FALSE, results = FALSE}
# Load required packages or install and load if not installed
packages <- c("data.table", "dslabs", "dplyr", "ggplot2")
fun_check_packages <- function(x){
  # require returns warning (library returns error) and implicit True/False 
  # statement depending on whether package is available or not
  # character.onlyenables use of character vector as object
  if(!require(x, character.only = TRUE)){
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
}
lapply(packages, fun_check_packages)

pathProj <- paste0("C:/Users/User/Documents/Projects/Data Science Projects/", 
                   "statistical-power-analysis/")
pathCode <- paste0(pathProj, "code/")
```

```{r 2-import-data-functions}
data("murders")
dtMurders <- 
  as.data.table(murders)[grepl("south|North", region, ignore.case = TRUE), ]
dtMurders[grepl("North", region, ignore.case = TRUE), region := "North"]
# Calculate murder rate per million to adjust for exposure
dtMurders[, Rate := 1000000 * total / population]

# Import functions- NOTE order is important since results-function is used inside 
# some of the other functions
list_functions <- c("results-function", "diff-means-function", "gamma-function", 
                     "randomization-function", 
                    "sample-size-function")
for (i in list_functions) {
  source(paste0(pathCode, i, ".R"))
}
```

# 1 Introduction

This report assesses how the size and power for the two-sample t-test and randomization test for the means of two samples vary under different scenarios. The three scenarios considered are changes in the sample size of test data, changes in the proportional difference in the means between the two samples and changes in the distributional form of the input data. In particular, given the nature of murder rates we consider that they have a gamma distribution with varying parameters. It is seen that the non-parametric randomization test is more robust when the data are no longer normal only at very small sample sizes (less than five for each group), however, the size and power of the two-sample t-test performs well for small samples and changes in the difference in means between the two regions.

This report assesses how the size and power of a parametric (two-sample t-test) and non-parametric (randomization) test changes when the two samples of interest have changing properties. Those properties are changes in the sample size of the simulated samples, changes in the effect size (difference) between the means of the two samples and the consideration of gamma (non-normal) data. The context of this report is to consider whether murder rates are different between Southern and Northern states in the US. In particular, states in the North-Eastern region largely do not impose the death penalty for homicide and have lower rates of unemployment and poverty compared to the Southern States (Death Penalty Information Centre, 2019). The data considered is the murder rate per thousand in each state. 

In particular I would like to know whether my prior expectation of the murder rate being higher in Southern US states compared to Northern states is reasonable.

# 2 Methodology

A parametric and non-parametric test are used to test whether there is a difference in means for the success rates for the two genders. For the given research question, the null hypothesis is that there is no difference between the success rates for men and women, whilst the alternative hypothesis is that there is a difference in means. This represents a two-sided test.

## 2.1 Two-Sample t-test

Because the data are not paired, we consider a two-sample t-test to assess whether the means of the two regions; South and North East, are different. The two-sample t-test has three key assumptions:
	The observations are independent
	Data and sample means are normally distributed
	Data from the two samples have equal variances
For large samples, the two-sample t-test is reasonably robust to deviances from normality.

## 2.2 Randomization Test

Randomization tests tend to be more useful when we have fewer parameters but here we have a sample of 1000 items. We perform two-sided randomization tests. Under H_0, all permutations of the data are equally likely. That is, under the null, both regions have the same distiribution, hence we can swap their labels for 

given murder rates and calculate a t-distribution for the t-statistics under this null. The purpose of the randomization test is to generate a distribution of the t-statistic under the null hypothesis that there is no difference in mean murder rate of the two regions. Randomization tests may be particularly useful when the distribution of the data is strongly different to that of a normal distribution and the sample size is small.

Permuting the data is equivalent to shuffle all labels for the groups in which observations lie in around. 

__Randomization Test Algorithm__

1) Define K as the number of permutations
2) for k = 1, ..., K
  2.1) Sample a permutation of the data
  2.2) Calculate the test statistic $T_k^{(R)}$ for this permutation
3) Count the number of permutations, $m$, for which $T_k^{(R)} \geq T^{(Obs)}$
4) p-value = $\frac{m}{K}$


## 2.3 Size and Power

The murder rates data for the Southern and North Eastern regions have means of 0.04417 and 0.01848 and variances of 0.001138 and 0.000138, respectively. Given the low variance size which may induce the power of both tests to always be one, we consider a much greater variance of 0.1 for each sample so that there is clear ‘overlap’ in the distributions of murder rates for each region.
We therefore start by assuming a somewhat unrealistic normal distribution given below:
X_(i,S)  ~ N(0.04417,0.1) and X_(i,NE)  ~ N(0.01848,0.1)
We now calculate the size and power of our parametric and non-parametric tests by numerical integration. This involves evaluating each test for many samples and calculating the proportion of tests in which the p-value is less than the significance level, α (in this paper we consider the standard significance level of 5%). Incorrectly rejecting H_0 when the null hypothesis is actually true is referred to as a “type 1 error” and is known as the size of the test. Given this, one would expect the size of the test to be similar to the significance level used.
Conversely, the power of a test is the probability that we reject H_0 given that the null is false. Therefore, we would like tests that exhibit high power. The size and power of these tests will vary, depending on 

To help interpretability of the data we consider the murder rate per million.

We simulate data under different scenarios to create a sample and then do this 1000 times to create 1000 samples. For each sample, we carry out the parametric and non-parametric tests and store the results in some matrix with 2 columns and 1000 rows. From this we then calculate the proportion of tests aht have type 1 and type 2 errors (i.e.- the size and power of these tests)

# 3 Results

Given that repeating the randomization function many times can be computationally time-consuming, we consider randomization tests of 200 samples and repeat the tests 1000 times.

## 3.1 Scenario 1 – Change the Sample Size

Smaller samples may reduce the power of tests to reject the null. Such samples may contain values far in the tails of the respective distributions and thus affect the size and power of these tests. We therefore consider the size and power of the two tests for samples of increasing size.

Table 1- Size and Power for Increasing Sample Size
Sample Size	Randomization Size	Randomization Power	t-test Size	t-test Power
5	0.061	0.077	0.040	0.050
10	0.052	0.085	0.050	0.084
20	0.046	0.124	0.046	0.130
50	0.034	0.238	0.024	0.232
100	0.054	0.424	0.048	0.422

200	0.040	0.738	0.040	0.740
500	0.044	0.974	0.042	0.978
1000	0.064	1.000	0.066	1.000

We see that the power of the randomization sample increases as the number of randomizaed sample we calculate increases. Here we only simulate for randomized samples of size 500 given the computational time required for this process.

In this case, we consider differing scenarios in which the 
When considering the test size and power, we should consider that repetitions of 1000 are relatively low for testing the true power and size.

Effect Size	Relative Effect Size	Randomization Power	t-test Power
0	0	0.055	0.055
0.0025		0.088	0.089
0.005		0.178	0.181
0.0075		0.374	0.380
0.01		0.594	0.601
0.0125		0.794	0.809
0.015		0.907	0.918
0.0175		0.967	0.970
0.02		0.996	0.995

For this question, it could be that there is a strong re-distribution of the murder rates between years and that 2010 is not representative of the long-run distributions in murder rates for each region. For example, it could be that states in the North East regions had a particularly low murder rate in 2010 compared to the long-run distribution of murder rates for each region. In this case, we may have a sample for 2010 that is at the tails of the long-run distribution for murder rates. The change in power as the difference in means increases is essentially what we would see if the variance of each sample was decreasing. In this case we get a test size of 0.057 for both the randomization test and two-sample t-test.

So far, we have simulated murder rates for both regions using normal distributions. However, given the nature and distribution of murder rates (figure ?), it is clear that a normal distribution is not appropriate 

because murder rates are always positive and tend to have a more right-skewed distribution. We therefore simulate murder rates from a gamma distribution to understand how the size and power of the two tests are affected when the data is no longer normal.
The gamma distribution is defined as:
f(x)=β^α/(Γ(α)) x^(α-1) e^(-βx)
Where x>0 and α>0,β>0. In this case “alpha” represents the shape of the distribution and “beta” represents the rate of the distribution. Importantly, the mean and variance of a gamma distribution is given by:
E[x]=α/β,Var[x]=  α/β^2 
In particular, for given gamma parameters, we assess how the sample size affects the power and size of each test. However, if the sample size is sufficiently large, it will not matter if the data violates normality because, by the central limit theorem, the means will be normally distributed. In this case the sampling distribution for the t-statistic will be standard normally distributed and p-values will be valid. We find that this is generally the case for samples of size as low as 10. The more unusual the distribution of the data is and the less it resembles a normal distribution, the larger the sample size needs to be to ensure a stable test size and high power.

Table 3- Power and size for data simulated under different probability distributions
Gamma Parameters	Randomization Size	Randomization Power	t-test Size	t-test Power
Alpha= , beta=				
Alpha= , beta=				
Alpha=, beta=				
50				
100				
200				
500				
1000			

Problem with T-tests is they are _Parametric_ and therefore require assumptions about the distribution of the data (e.g. Normality) and assumptions about the distribution of the test statistic under the Null hypothesis (derived from assumptions about the distribution of the data).

Randomization tests are similar to _Permutation tests_. Under a permutation test, we permute the values across the 2 groups, since under our null hypothesis there is no difference in the value of interest. We then calculate the test statistic for each permutation, giving us a distribution of the test statistic under the Null hypothesis. Randomization tests are like permutation tests, except that only a random subset of all the possible permutations are generated.
We get an approximate distribution of the test statistc that is non-parametric hence requires no assumptions about the data.

In particular, given the shape of the histogram when plotting murder rates, we use variants of the gamma distribution, including the chi-squared distribution to assess how the power and size of these tests alter when the data have different distirbutions.
Whilst the t-test may not be valid for non-normal data, if the sample size is large enough then the t-test will be robust due to the central limit theorem. If the data is non-normal, then 
Generally, we see that the two-sided t-test tends to outperform the randomization test, except when the data is gamma distributed and no longer normal. Other methods could also have been used to assess the small sample properties of the two tests when the variance of each sample is different, when the variance increases so that there is greater overlap in the distributions of each region, or if other non-normal distributions were used to simulate the data.

Randomization tests can be extended to a plethora of scenarios. 

<div class="alert alert-info">
  <strong>Is Normality a Reasonable Assumption?</strong> 
  
  In many cases it may not intuitively feel as though our data is normally distibuted for this assumption in the Two-Sample T-Test to seem feasible. However, the _Central Limit Theorem_ ( __CLT__ ) tells us that the sample mean $\bar{X}_n$ of a large sample of random variables with mean $\mu$ and finite variance $\sigma^2$ has approximately a normal distribution with mean $\mu$ and variance $\frac{\sigma^2}{n}$.
  
  This result helps to justify the use of the normal distribution as a model for many variables that can be thought of as being made up of many independent parts. The CLT is therefore a formal statement of how normal distributions can approximate distributions of general sums or averages of i.i.d. radm variables. For example, a person's height is influenced by many random factors. If the height of each person is determined by adding the values of these individual factors, then the distribution of the heights of a large number of persons will be approximately normally distributed. In general, the CLT indictes that the distribution of the sum of many random variables can be approximately normal, even though the distribtuion of each random variable in the sum differs from normality.
</div>

<div class="alert alert-info">
  <strong>The caveats of p-values</strong> 
  
  A _p-value_ is the samllest level $\alpha_0$ such that we would reject the null-hypothesis at level $\alpha_0$ with the observed data. This is the probability of seeing a test statistic at least as extreme as the one observed, under the Null hypothesis that the test statistic came from the null distribution. An experimenter who rejects a null hypothesis iff the p-value is at most $\alpha_0$ is using a test with level of significance $\alpha_0$.
  
  The choice of significance threshold, $\alpha_0$, for rejection of the Null hypothesis is arbitrary. Typically we take $\alpha_0 = 0.05$ but by definition this yields a type-1 error (incorrectly reject the null hypothesis) with probability 0.05. 
</div>


<div class="alert alert-info">
  <strong>Central Limit Theorem when the Support is not the Whole Real Line</strong> 
  
  Naturally the murder rate cannot be negative. Therefore, the mean of the murder rate cannot be negative and must have the same support as the murder rate itself. This implies the assumption of a Normal distribution for $\bar{X}_n$ is invalid since the Normal distribution can have negative support. 
 
</div>


# 4 Conclusion

This report has illustrated how test power and size change for the two tests as the samples upon which they act alter.

# 5 References

Statistics How To. (2019). Parametric and Non -Parametric Data. Retrieved from https://www.statisticshowto.datasciencecentral.com/parametric-and-non-parametric-data/.

Death Penalty Information Centre. (2019). State-by-State. Retrieved from https://deathpenaltyinfo.org/state-and-federal-info/state-by-state.

RStudio Team (2016). RStudio: Integrated Development for R. RStudio, Inc., Boston, MA URL http://www.rstudio.com/.

Kutner, M., Li, W., Nachtsheim, C., Neter, J. (2013). Applied Linear Statistical Models. McGraw Hill Education.

Glennie, R. (2019). MT4113- Computing in Statistics. [Lecture Notes] University of St Andrews.

Bartlett, J. (September 28, 2013). The t-test and Robustness to Non-Normality. Retrieved from: https://thestatsgeek.com/2013/09/28/the-t-test-and-robustness-to-non-normality/
Lehman, E. (1999). Elements of Large Sample Theory. Springer-Verlag, New York.

DeGroot, M., Schervish, M. (2012). Probability and Statistics (4th Ed.). Pearson Education.

# Exploratory Data Analysis

Simply calculating the mean of . Naturally we could increase the sample size and hence the power fo the test by simply considering that the number of murders in each state is the sum of murders for the counties that constitute each state. 

https://stats.stackexchange.com/questions/339759/how-can-the-central-limit-theorem-hold-for-distributions-which-have-limits-on-th

A Normal distribution actually seems a reasonable assumption. The number of murders for each state is the addition of the number of murders at much more local levels, such as by county. Many factors will also impact on the number of murders. Therefore the total number of murders across states can be seen as the sum of many individual factors, therefore it is reasonable to assume an approximately normal distribution Of course, one caveat of this is that we can't have negative murders and by the continuous nature of the normal distribution, this will always be theoretically possible, even for a large mean estimate and small variance. 

When is the data from, i.e. what time period and what is the original, credible source of the data?

```{r}
#We consider the distributional properties of the original murder rates data
#to inform the properties of the simulated data
#Plot the distribution of the original data for each region
cSouthRate <- dtMurders[region == "South", ]$Rate
cNorthRate <- dtMurders[region == "North", ]$Rate
hist(cSouthRate, xlab = "Murder Rate", 
     main = "South Region Murder Rate Distribution")
hist(cNorthRate, xlab = "Murder Rate", main = "North East Region Murder
     Rate Distribution")
#Mean murder rates for each region
mean(cSouthRate)
mean(cNorthRate)
#Variance of murder rates for each region
var(cSouthRate)
var(cNorthRate)
# kable these summary statistics together into a table

#1) Scenario 1

#Use the sampSizeFun function to calculate power and size for samples of
#increasing size
n2 <- sampSizeFun(N = 2, K = 200, R = 500)
n3 <- sampSizeFun(N = 3, K = 200, R = 500)
n4 <- sampSizeFun(N = 4, K = 200, R = 500)
n5 <- sampSizeFun(N = 5, K = 200, R = 500)
n10 <- sampSizeFun(N = 10, K = 200, R = 500)
n20 <- sampSizeFun(N = 20, K = 200, R = 500)
n50 <- sampSizeFun(N = 50, K = 200, R = 500)
n100 <- sampSizeFun(N = 100, K= 200, R = 500)
n200 <- sampSizeFun(N = 200, K = 200, R = 500)
n500 <- sampSizeFun(N = 500, K = 200, R = 500)
n1000 <- sampSizeFun(N = 1000, K = 200, R = 500)

#Plot sample size and Power for both tests
#vector of sample sizes
N <- c(5, 10, 20, 50, 100, 200, 500, 1000)
#dataframe of test powers and sizes
SampRes <- rbind(n5, n10, n20, n50, n100, n200, n500, n1000)
SampRes
#Plot of test power against sample size
ggplot() + geom_point(aes(x = N, y = SampRes$rPower, 
  col = 'Randomization Test'), size = 3) + geom_point(aes(x = N, 
  y = SampRes$tPower, col = 't-test'), size = 3) + 
  labs(title = 'Test Power for Increasing Sample Size', x = 'Sample Size', 
  y = 'Power') + geom_line(aes(x = N, y = SampRes$rPower, 
  col = 'Randomization Test'), size = 1.2) + geom_line(aes(x = N, 
  y = SampRes$tPower, col = 't-test'), size = 1.2)
#Plot of test size against sample size
ggplot() + 
  geom_point(aes(x = N, y = SampRes$rSize, col = 'Randomization Test'), size = 2) + 
  geom_point(aes(x = N, y = SampRes$tSize, col = 't-test'), size = 2) + 
  labs(title = 'Test Size for Increasing Sample Size', x = 'Sample Size', 
       y = 'Test Size') + 
  geom_line(aes(x = N, y = SampRes$rSize, col = 'Randomization Test'), size = 1) + 
  geom_line(aes(x = N, y = SampRes$tSize, col = 't-test'), size = 1)

#2) Scenario 2

#Use the diffMeansFun function to calculate power and size for samples 
#with increasing differences in means
D0 <- diffMeansFun(diff = 0, K= 200, R= 1000)
D1 <- diffMeansFun(diff = 0.0025, K = 200, R = 1000)
D2 <- diffMeansFun(diff = 0.005, K = 200, R = 1000)
D3 <- diffMeansFun(diff = 0.0075, K = 200, R = 1000)
D4 <- diffMeansFun(diff = 0.01, K = 200, R = 1000)
D5 <- diffMeansFun(diff = 0.0125, K = 200, R = 1000)
D6 <- diffMeansFun(diff = 0.015, K = 200, R = 1000)
D7 <- diffMeansFun(diff = 0.0175, K = 200, R = 1000)
D8 <- diffMeansFun(diff = 0.02, K = 200, R = 1000)
#vector of mean differences
S <- c(0.0, 0.0025, 0.005, 0.0075, 0.01, 0.0125, 0.015, 0.0175, 0.02)
#dataframe of test powers and sizes
diffRes <- rbind(D0, D1, D2, D3, D4, D5, D6, D7, D8)
diffRes
#Plot how power changes with increasing effect size
ggplot() + 
  geom_point(aes(x = S, y = diffRes$rPower, col = 'Randomization Test'), size = 2) + 
  geom_point(aes(x = S, y = diffRes$tPower, col = 't-test'), size = 2) + 
  labs(title = 'Test Power for Increasing Effect Size', x = 'Sample Size', 
    y = 'Power') + 
  geom_line(aes(x = S, y = diffRes$rPower, col = 'Randomization Test'), size = 1) + 
  geom_line(aes(x = S, y = diffRes$tPower, col = 't-test'), size = 1)


#3) Scenario 3

#Apply the function to Gamma Distributions with different parameters
G1 <- gammaFun(par1 = c(4, 100),par2 = c(2, 100), K = 100, R = 1000, N = 10)
G2 <- gammaFun(par1 = c(4, 100),par2 = c(2, 100), K = 100, R = 1000, N = 100)
G3 <- gammaFun(par1 = c(4, 100),par2 = c(2, 100), K = 100, R = 1000, N = 1000)
G4 <- gammaFun(par1 = c(3.2, 100),par2 = c(2.8, 100), K = 100, R = 1000, N = 10)
G5 <- gammaFun(par1 = c(3.2, 100),par2 = c(2.8, 100), K = 100, R = 1000, N = 100)
G6 <- gammaFun(par1 = c(3.2, 100),par2 = c(2.8, 100), K = 100, R = 1000, N = 1000)

#Collect the results in a dataframe for comparison
gammaResults <- rbind(G1, G2, G3, G4, G5, G6)
gammaResults
#Plot each simulated gamma distribution against the South data
hist(SouthData$rate, xlab = "Murder Rate", main = "South Region Murder Rate 
     Distribution", freq = FALSE)
lines(density(rgamma(10000, 4.417, 100)), col = 'blue')
lines(density(rgamma(10000, 5, 100)), col = 'red')
lines(density(rgamma(10000, 4.417, 110)), col = 'green')
#Plot each simulated gamma distribution against the North East data
hist(NorthData$rate, xlab = "Murder Rate", main = "North East Region Murder
     Rate Distribution", freq = FALSE)
lines(density(rgamma(10000, 1.848, 100)), col = 'blue')
lines(density(rgamma(10000, 2, 100)), col = 'red')
lines(density(rgamma(10000, 1.848, 110)), col = 'green')
#Graph to show changes in the two simulated gamma distributions
plot(density(rgamma(10000, 4.417, 100)), col = 'blue', xlab = 'Murder Rate', 
  main = 'Different Gamma Distributions Simulated for the Regions', 
  xlim = c(0, 0.12), ylim = c(0, 45), lwd = 2)
lines(density(rgamma(10000, 5, 100)), col = 'red', lwd = 2)
lines(density(rgamma(10000, 4.417, 110)), col = 'green', lwd = 2)
lines(density(rgamma(10000, 1.848, 100)), col = 'blue', lty = 2, lwd = 2)
lines(density(rgamma(10000, 2, 100)), col = 'red', lty = 2, lwd = 2)
lines(density(rgamma(10000, 1.1, 48)), col = 'green', lty = 2, lwd = 2)
```